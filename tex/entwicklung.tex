%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Zusammenfassung und Ausblick
\section{Entwicklung}
\label{sec:entwicklung}
Dieses Kapitel befasst sich mit einzelnen Aspekten der Entwicklung. Nach der Auflistung der Nicht-funktionalen Anforderungen, sowie der Funktionen, werden grundlegende Konzepte und Architekturen vorgestellt. Dabei werden erste Sensoren, Aktoren und bestehende Software Bibliotheken getestet und für eine Anschlussverwendung bewertet.

\subsection{Nicht-Funktionale Anforderungen}
\label{sec:dev-nichtfunk}

Die nicht-funktionalen Anforderungen beschreiben Qualitätsmerkmale des MRS. Diese sind in verschiedene Kategorien gruppiert und haben Auswirkungen auf die Funktionalität und die Architektur. Im Folgenden werden nur die für diese Arbeit relevanten Kategorien erläutert und in Bezug gebracht.

\subsubsection{Sicherheit - Safety}

Im englischen wird der Begriff Sicherheit in die Terme \textit{Security} und \textit{Safety} unterteilt. Ersteres beschäftigt sich unter anderem mit der Verschlüsselung von Daten. Der Begriff \textit{Safety} bezieht sich auf die Anwendersicherheit, zum Beispiel ob dieser durch das System Verletzungen erleiden kann oder ob sich das System selbst oder die Umwelt beschädigt. Da die Roboter in dieser Arbeit nicht, wie Industrieroboter, durch Gitterzäune gesichert sind wird eine Lichtwarnanlage installiert. Diese arbeitet ähnlich einer Ampel. Befindet sich das System in einer sicheren Stellung, in der es sich nicht bewegt, leuchtet die Lichtanlage in grün. Wird eine Bewegung geplant oder ist das System vor einer Bewegung blinkt die Lichtanlage fünf Sekunden gelb. Innerhalb dieses Intervalls bewegt sich das System nicht. Auf das Blinken folgt ein rotes Leuchten der Lichtanlage. In dieser Zeit führen die Roboter ihre Aktionen aus. Während dieser Aktionen nutzt das Robotersystem die Sensoren und berechnet kollisionsfreie Pfade um nicht mit sich selbst oder der Umwelt zu kollidieren. Objekte, die sich erst zur Laufzeit auf dem Pfad befinden, werden in dieser Arbeit noch nicht beachtet und können so zur Kollision führen. Um bei Bewegungen der mobilen Plattform oder zwischen zwei Tasks der Arme die Sicherheit dieser zu sicheren existieren die beiden Posen \textit{Candle} und \textit{Fold}. Die erste Pose streckt den Arm senkrecht in die Höhe und ermöglicht so kollisionsfreie Pfade in die meisten anderen Posen. In der zweiten Pose befinden sich alle Gelenke in ihrer minimalen Konfiguration. Sollte dem Roboter der Strom fehlen, bedingt durch einen Stromausfall oder leere Batterien, fällt der Roboterarm in sich zusammen, da die Gelenkmotoren die Position nicht mehr halten können. In der \textit{Fold}-Pose kann der Roboter nicht weiter in sich zusammenfallen, da alle Glieder aufeinander liegen. Deshalb sollten die Roboter diese Pose in längeren Inaktivitätsphasen einnehmen. Diese Aspekte werden während der Entwicklung der Funktionen berücksichtigt.

\subsubsection{Zuverlässigkeit}
Die Zuverlässigkeit des Systems beschreibt unter anderem das Systemverhalten bei einem Fehlerfall oder Teilsystemausfall. Das Robotersystem darf dabei nicht in einen unsicheren Zustand geraten. Da das Robotersystem auf ROS aufbaut werden einzelne Aktoren und Sensoren als ROS-Node betrieben. Jeder Node stellt dabei ein einzelnes Subsystem dar, welcher ausfallen kann. ROS ist so konzipiert, dass das ganze System nicht abstürzt, wenn ein einzelner Node ausfällt. Eine Schwachstelle bildet hierbei der ROS-Core. Dieser stellt den Mittelpunkt eines ROS-Systems dar. Fällt dieser aus bricht die Kommunikation zwischen den einzelnen Nodes zusammen. In diesem Fall müssen die Nodes in einen sicheren Fehlerzustand übergehen. Dies betrifft vor allem die Aktoren, besonders die Steuerungen der Roboter. 

Ein weiterer Aspekt der Zuverlässigkeit ist die Wiederherstellbarkeit. Dies betrifft ob und wie komplex ein bestimmter fehlerfreier Systemstatus wiederhergestellt werden kann. Da die Nodes einzelne Subsysteme sind, können diese nach einem Ausfall einfach wieder gestartet werden und der Systemstatus ist wiederhergestellt. Dies betrifft auch den ROS-Core. Wird dieser auf der selben Adresse (IP und Port) neu gestartet, nehmen alle Nodes die Kommunikation wieder auf. Diese beiden Aspekte werden vor allem bei der Entwicklung der Architektur für das MRS berücksichtigt. Dabei ist das zentrale Koordinierungs- und Konfigurations-System eine Komponente, die zu einem potenziellen Single-Point-of-Failure führen kann.


\subsubsection{Korrektheit}
Die Korrektheit lässt sich in dieser Entwicklung durch die Genauigkeit der Funktionen definieren. So dürfen die Positionen und Orientierungen der Arme und der mobilen Plattform nur um bestimmte Grenzen abweichen. Dabei gelten für die End-Effektor-Pose eine Distanzabweichung von maximal 5mm für jede Dimension und für die Orientierung eine Abweichung von maximal 1\textdegree pro Achse. Dies muss bei der Entwicklung der inversen Kinematik berücksichtigt werden. Für die mobile Plattform gelten andere maximale Abweichungen. Für die Distanzabweichungen sind das 5cm in X- und Y-Richtung. Für die Z-Dimension darf sie maximal 5mm sein. Für die Rotation gilt: Z-Achse 5\textdegree, X- und Y- Achse 1\textdegree.

\subsubsection{Leistung}
Die Leistung bezieht sich auf die Effizienz des MRS. Besonders der Zeit- und Energieaufwand sollen möglichst minimiert werden. Der Zeitaufwand bezieht sich dabei nicht auf die Performance-Optimierung der Algorithmen, sondern der Bewegungen und deren Abläufe. Dieser Aspekt muss bei der Entwicklung der Funktionen und der Implementierung berücksichtigt werden. Der Energieaufwand ist stark abhängig von den benutzten Aktoren und wird bei der Entwicklung der inversen Kinematik für die Arme und die Pfadbestimmung für die mobile Plattform in Betracht gezogen.

\subsubsection{Änderbarkeit}
Die Änderbarkeit besteht aus Sicht der Entwicklung aus mehreren Aspekten. Für diese Arbeit ist dabei die Erweiterbarkeit ein wichtiger Aspekt. Zum einen geht es um die Erweiterbarkeit des MRS um weitere Sensoren und Aktoren. Diese sollen möglichst einfach in das bestehende MRS eingebunden werden können ohne bestehende Subsysteme zu verändern. Zum anderen soll in der Weiterentwicklung dieses MRS weitere Konzepte eingebunden werden. So müssen für eine automatische Konfiguration und verteilte Koordinierung Schnittstellen in der Architektur entworfen werfen. Diese Schnittstellen sollen unter anderem eine Markt-basierte Konfiguration ermöglichen. Diese beiden Aspekte der Erweiterbarkeit müssen in der Entwicklung der Architektur und der Funktionen berücksichtigt werden. So sind grobe Kostenangaben für die Funktionen ein möglicher Ansatz für die Gebote am Markt.

\subsection{Funktionalitäten}
\label{sec:dev-funk}

In diesem Kapitel werden die Funktionen des Robotersystem aufgelistet. Das folgende Use-Case Diagramm \ref{fig:dev-usecase} stellt die groben Funktionsanforderungen an das Robotersystem dar.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.8]{fig/UseCase}   
	\caption[Use-Case Robotersystem]{Vereinfachtes Use-Case Diagramm zur groben Funktionsübersicht des Robotersystems}
	\label{fig:dev-usecase}
\end{figure}

Der Anwender gibt dem System die Anweisung einen Gegenstand wegzuräumen. Das Robotersystem gliedert die Anweisung in einzelne Tasks auf. Diese Tasks werden von den unterschiedlichen Akteuren mit ihren Funktionalitäten sequentiell oder parallel ausgeführt. In diesem konkreten Fall soll zunächst der Roboter Rose den entsprechenden Gegenstand aufheben. Dies erfordert zunächst eine Lokalisierung des Objektes die von einem Kamerasystem angefordert wird. Anschließend übergeben sich die Roboter Rose und Dummy den Gegenstand, bevor Dummy ihn anschließend an einer gewünschten Position ablegt. Diese sehr grobe Darstellung des Vorgangs beinhaltet die vier zentralen Funktionen, die das System umsetzen muss: Gegenstand aufheben (\textit{pick-up}), Gegenstand ablegen (\textit{place}), Gegenstand lokalisieren und Gegenstand übergeben(\textit{handover}).

 Die folgenden Anforderungen dienen zur Strukturierung der Arbeit und zerlegen das Robotersystem in Teilsysteme. Dies wird benötigt um einen Aufwand für die Koordinierung und Konfiguration zu bestimmen. Deshalb sind die Anforderungen grobe Darstellungen und aus dem Aspekt der Software-Entwicklung nicht detailliert genug. Dafür müssten die Funktionen weiter runter gebrochen werden. Dies würde jedoch den Umfang dieser wissenschaftlichen Arbeit überschreiten.

Alle Funktionen werden nach dem Schema aus \cite{lundh2006plan} angegeben: 

$f = \langle  Id, r, I, O, \Phi, Pr, Po, Freq, Cost\rangle$

Jede Funktion $f$ entspricht dabei einem Tupel und ist einem oder mehreren Agenten im System zugewiesen. Diese Zuweisung wird durch das Feld $r$ abgebildet und im anschließen Kapitel \ref{sec:agent} eingetragen. Die weiteren Felder des Tupels stehen für:

\begin{description}
	\item [$Id$] Eindeutiger Bezeichner für die Funktion.
	\item [$I = \{i_1, i_2,\ldots,i_n\}$] Eine Menge aller Eingangsparameter.
	\item [$O = \{o_1, o_2,\ldots,o_n\}$] Eine Menge aller Ausgangsparameter.
	\item[$\Phi$] Definiert den Übergang zwischen Eingabe und Ausgabe.
	\item [$Pr$] Zustand $s \in S$ aus dem diese Funktion gestartet werden kann.
	\item [$Po : S \times I \rightarrow S$] Eine Funktion die den Folgezustand $s\textasciiacute$ anhand des Inputs und des Startzustandes $s$ angibt.
	\item[$Freq$] Frequenz in der die Funktionalität ausgeführt werden soll.
	\item[$Cost$] Kosten der Ausführung, zum Beispiel Zeit oder Energie. Können konstant aber auch funktionell angegeben werden.
\end{description}

Je nach Typ des Agenten sind die Eingabemenge bei Sensoren $I = \varnothing$ und die Ausgabemenge bei Aktoren $O =  \varnothing$. Außerdem wird jede Funktion beschrieben und für die Verwendung in ein MRS eingeordnet. Aus dem Anwendungsfall für das Robotersystem ergeben sich folgende funktionale Anforderungen.

\subsubsection{Funktion Aufheben}

\begin{tabular}{|p{3cm}|p{10cm}|}
	\multicolumn{2}{c}{	$f_0$ (Aufheben):}\\
	\hline  $Id$ & pickup\\ 
	\hline  $I$ & Objektposition im globalen oder lokalen Koordinatensystem. \\ 
	\hline  $O$ & Erfolgsmeldung. \\ 
	\hline  $\Phi$ & Wenn die Objektposition durch den Agenten erreichbar und der Agent voll funktionsfähig ist erfolgt eine positive Rückmeldung. Ansonsten eine negative Erfolgsmeldung.\\ 
	\hline $Pr$ & Zustand $s_1$, Objekt im Nahfeld gefunden und Arm in Candle-Stellung. \\ 
	\hline $Po$ & Zustand $s_2$, Arm in Candle-Stellung und Objekt gegriffen, oder Fehlerzustand $s_e$ bei Fehlschlag. \\ 
	\hline $Freq$ & Einmalige Funktion. \\ 
	\hline $Cost$ & Dynamisch zu ermitteln. Abhängig von Weg und Erreichbarkeit des Objektes. \\
	\hline
\end{tabular}

\paragraph{Beschreibung}
Die Funktion \textit{pickup} wird in den beiden Robotern Rose und Dummy implementiert und wird für das Aufheben von Objekten genutzt. Beschränkt wird diese Funktionalität durch die Erreichbarkeit der Objektposition. Gerade für den stationären Dummy ist der Arbeitsraum stark eingeschränkt. Rose hat die Möglichkeit während dieser Funktionalität die eigene Position zu verändern um das Objekt zu erreichen. Das Aufheben bedingt, dass das Objekt auf einer parallelen Ebenen zur X-Y-Ebene liegt, da die Greifer Z-Achse parallel zur globalen Z-Achse greift. Die Rotation um die Z-Achse des Objektes ist dabei frei wählbar, da diese durch eine Rotation des Greifers ausgeglichen werden kann. Damit der Roboter nicht auf dem Weg zum Objekt dasselbe berührt wird eine Position über dem Objekt angefahren, bevor der Greifer an der globalen Z-Achse hin abfährt.  Diese Beschränkung reduziert die Arbeitsräume der Roboterarme, da ein senkrechter Griff die Zahl der möglichen Gelenkpositionen stark einschränkt und jede Position auf dem senkrechten Pfad erreichbar sein muss. Eine nicht erreichbare Position führt zum Abbruch der Funktion. Eine anschließende sichere Konfiguration des Roboterarms ist nicht gewährleistet. Für Rose gilt des Weiteren auch ein undefinierter Standort nach Abbruch. Wird die Funktion erfolgreich beendet befindet sich der Arm in einer sicheren Konfiguration (\textit{Candle}- oder \textit{Fold}-Pose).

\paragraph{Einordnung}
Die Funktion ist als Single-Robot Task für einen Single-Task-fähigen Agenten einzuordnen, da nur ein Agent zur Erledigung nötig ist. Die Funktion kann von einem Multi-Task fähigen Agenten intern auch parallel ausgeführt werden, da einzelne Subtasks unabhängig von einander erledigt werden können. Ein Beispiel ist das Öffnen des Greifers während der Bewegungsphase des restlichen Armes oder der mobilen Plattform bei Rose. Diese Multi-Task Anwendung fordert aber einen höheren Koordinierungsaufwand, da Funktionen zeitlich voneinander abhängen. So darf sich der Arm erst auf das Objekt zubewegen, wenn der Greifer geöffnet ist. Für eine Single-Task Anwendung ist der Koordinierungsaufwand gering, da alle Subtasks seriell ausgeführt werden. Der Konfigurationsaufwand ist mittel, da zwei Agenten für die Funktionalität bereitstehen, welche sich über den Aktionsradius für die Ausführung qualifizieren. 

\subsubsection{Funktion Ablegen}

\begin{tabular}{|p{3cm}|p{10cm}|}
	\multicolumn{2}{c}{$f_1$ (Ablegen):}\\
	\hline  $Id$ & place\\ 
	\hline  $I$ & Zielposition im globalen oder lokalen Koordinatensystem. \\ 
	\hline  $O$ & Erfolgsmeldung. \\ 
	\hline  $\Phi$ & Wenn die Zielposition durch den Agenten erreichbar und der Agent voll funktionsfähig ist erfolgt eine positive Rückmeldung. Ansonsten eine negative Erfolgsmeldung.\\ 
	\hline $Pr$ & Zustand $s_2$, Arm in Candle-Stellung und Objekt gegriffen. \\ 
	\hline $Po$ & Ruhezustand $s_r$, Arm in Candle-Stellung oder Fehlerzustand $s_e$ bei Fehlschlag. \\ 
	\hline $Freq$ & Einmalige Funktion.\\ 
	\hline $Cost$ & Dynamisch zu ermitteln. Abhängig von Weg und Erreichbarkeit der Zielpositionen. \\
	\hline
\end{tabular}

\paragraph{Beschreibung}
Ähnlich Funktion $f_0$ wird diese Funktionalität den beiden Robotern Dummy und Rose implementiert. Dabei legt der Arm ein Objekt an einer gewünschten Position ab. Beschränkt wird die Funktion auch durch den Arbeitsraum der einzelnen Roboter.  Das Objekt kann ebenfalls nur auf einer Ebene abgelegt werden, die parallel zur X-Y-Ebene ist. Die Rotation um die Z-Achse kann beim ablegen vorgegeben werden. im Gegensatz zu $f_0$ nähert sich der Greifer nicht dem Objekt an, sondern entfernt sich nach dem Ablegen positiv auf der globalen Z-Achse vom Objekt um dieses in einer anschließenden Bewegung nicht durch eine Berührung zu manipulieren. Bei einer erfolgreichen Ausführung beendet die Funktionalität befindet sich der Arm in einer sicheren Konfiguration (\textit{Candle}- oder \textit{Fold}-Pose). Bei einem Fehlschlag ist diese Konfiguration nicht garantiert.

\paragraph{Einordnung}
Die Funktion gilt als Single-Robot Task. Die Subtasks der Funktion können seriell als auch parallel ausgeführt werden. Dadurch eignen sie sich für Multi-Task, sowie Single-Task Agenten. Je nach Typ ist der Koordinierungsaufwand unterschiedlich hoch. Der Single-Task Agent benötigt wenig Koordinierung da nur ein Startsignal notwendig ist. Bei einem Multi-Task Agenten ist die Koordinierung aufwendiger, da Subtasks zeitlich voneinander abhängen. Zum Beispiel darf der Greifer erst öffnen, wenn das Objekt in der Position ist. Der Konfigurationsaufwand ist gering, da nur der Agent die Aktion ausführen kann, der den Gegenstand hält. 

\subsubsection{Funktion Objekt identifizieren und lokalisieren}
\label{sec:funraum}

\begin{tabular}{|p{3cm}|p{10cm}|}
	\multicolumn{2}{c}{$f_2$ (Objekt im Raum finden):}\\
	\hline  $Id$ & findObj\\ 
	\hline  $I$ & Gewünschte Objektdaten. \\ 
	\hline  $O$ & Position des Zielobjektes im globalen Koordinatensystem. \\ 
	\hline  $\Phi$ & Findet der Agent anhand der Objektdaten das Objekt wird die Position zurückgegeben. Ansonsten folgt eine Fehlermeldung.\\ 
	\hline $Pr$ & Ruhezustand $s_r$,Keine Vorbedingung nötig. \\ 
	\hline $Po$ & Zustand $s_0$, Objekt gefunden. Oder Fehlerzustand $s_e$\\ 
	\hline $Freq$ & Einmalige Funktion.\\ 
	\hline $Cost$ & Konstant. Abhängig von Prozessor der Recheneinheit, Algorithmus und Kamerasystem. \\
	\hline
\end{tabular} 

\paragraph{Beschreibung}
Diese Funktion identifiziert und lokalisiert ein bestimmtes Objekt im Großraum (Bodenfläche im Arbeitsraum größer als ein Quadratmeter). Je nach Typ der Identifizierung werden unterschiedliche Objektdaten benötigt. Eine Identifizierung basierend auf der Farbe des Objektes benötigt die Objektfarbe. Eine Identifizierung basierend auf der Form benötigt ein 3D-Modell oder eine Kostenfunktion für das Objekt. Die Identifizierung ist abhängig vom verwendeten Algorithmus. Dieser wird in Kapitel \ref{sec:detek} genauer erklärt. Findet die Funktion das gewünschte Objekt nicht oder tritt ein Fehler bei der Erkennung auf bricht die Funktion ab und gibt eine Fehlermeldung zurück. Bei einer erfolgreichen Identifizierung wird anhand von 3D-Daten die Position des Objektes berechnet. Bei dieser Lokalisierung soll eine Genauigkeit von fünf cm erreicht werden und damit nur eine grobe Position. Genauere Ergebnisse werden mit der Naherkennung $f_3$ erreicht. 

\paragraph{Einordnung}
Ein simpler Single-Robot Task, der von einem Agenten ausgeführt wird. Der Task wird in zwei Subtasks (Identifizieren und Lokalisieren) zerlegt, die serielle ausgeführt werden. Damit bringen Multi-Task Agenten keinen Vorteil. Der Koordinierungsaufwand ist sehr gering, da die Funktion nur angestoßen werden muss. Sollte die Funktion jedoch mit mehreren Kamerasystemen genutzt werden steigt der Koordinierungsaufwand, da der Task entweder parallel oder seriell von allen Systemen ausgeführt werden kann. Je nach Typ steigt der Konfigurationsaufwand, bei einer parallelen Ausführung ist kein Aufwand nötig, da immer alle Kamerasysteme genutzt werden. Eine serielle Ausführung würde die Komplexität steigen, da die Kamerasysteme in einer, möglicherweise priorisierten, Reihenfolge angestoßen werden.

\subsubsection{Funktion Nahfeld Erkennung}
\label{sec:funnah}

\begin{tabular}{|p{3cm}|p{10cm}|}
	\multicolumn{2}{c}{$f_3$ (Nahfeld Erkennung):}\\
	\hline  $Id$ & findObjNear\\ 
	\hline  $I$ & Gewünschte Objektdaten. \\ 
	\hline  $O$ & Position des Zielobjektes im lokalen Koordinatensystem. \\ 
	\hline  $\Phi$ & Findet der Agent anhand der Objektdaten das Objekt wird die Position zurückgegeben. Ansonsten folgt eine Fehlermeldung.\\ 
	\hline $Pr$ & Zustand $s_0$, Objekt im Raum gefunden und im Arbeitsraum der Kamera. \\ 
	\hline $Po$ & Zustand $s_1$, Objekt im Nahfeld gefunden. Oder Fehlerzustand $s_e$\\ 
	\hline $Freq$ & Einmalige Funktion.\\ 
	\hline $Cost$ & Konstant. Abhängig von Prozessor der Recheneinheit, Algorithmus und Kamerasystem. \\
	\hline
\end{tabular} 

\paragraph{Beschreibung}
Diese Funktion identifiziert und lokalisiert ein bestimmtes Objekt im Nahfeld (Bodenfläche im Arbeitsraum kleiner als ein Quadratmeter). Es gelten die gleichen Merkmale wie bei $f_2$. Der verwendete Algorithmus wird in \ref{sec:eye} genauer erklärt. Findet die Funktion das gewünschte Objekt nicht oder tritt ein Fehler bei der Erkennung auf bricht die Funktion ab und gibt eine Fehlermeldung zurück. Zusätzlich zur Position wird auch die Orientierung des Objektes berechnet. Die bestimmte Position entspricht dem Flächenschwerpunkt der Oberfläche des Objektes und soll eine Genauigkeit von fünf mm entsprechen.

\paragraph{Einordnung}
Wie $f_2$ einzuordnen. Kann jedoch nicht mit mehreren Kamerasystemen ausgeführt werden, da jedes Kamerasystem auf einem Roboter montiert und mit diesem eng-gekoppelt ist. Dadurch ist der Konfigurationsaufwand konstant, da jedem Roboter ein System zugewiesen wird. 

\subsubsection{Funktion Agentenlokalisierung}
\ref{sec:funl}
\begin{tabular}{|p{3cm}|p{10cm}|}
	\multicolumn{2}{c}{$f_4$ (Lokalisierung Agent):}\\
	\hline  $Id$ & loc\\ 
	\hline  $I$ & $\emptyset$ \\ 
	\hline  $O$ & Position des Agenten im globalen Koordinatensystem. \\ 
	\hline  $\Phi$ & Kann der Agent sich selbst im Raum lokalisieren, wird die Position zurückgegeben. Ansonsten folgt eine Fehlermeldung.\\ 
	\hline $Pr$ & In jedem Zustand $s_s$ möglich \\ 
	\hline $Po$ & Wie der Zustand beim Start $s_s$.\\ 
	\hline $Freq$ & Abhängig vom Sensor.\\ 
	\hline $Cost$ & Konstant. Abhängig von Prozessor der Recheneinheit, Algorithmus und Kamerasystem. \\
	\hline
\end{tabular} 

\paragraph{Beschreibung}
Diese Funktionalität lokalisiert den entsprechenden Agenten im Raum. Mögliche Algorithmen sind das Nutzen der Odometrie-Daten, bildbasierte Odometrie, bildbasierte Lokalisierung beruhend auf externen Sensoren und Funktionen ($f_2$) oder bildbasierte Lokalisierung basierend auf Merkmals-Erkennung. Es existieren noch weitere Methoden der Lokalisierung, dafür fehlen aber in dieser Arbeit die entsprechenden Sensoren und Aktoren.

\paragraph{Einordnung}
Eine an den Agenten enge-gekoppelte Funktionalität die an einen mobilen Roboter oder externen Sensor vergeben werden kann. Ein mobiler Roboter muss multi-tasking fähig sein, da diese Funktion in einer bestimmten Frequenz wiederholt wird und von anderen Funktionen benötigt wird. Ein Beispiel ist $f_0$, bei dieser Funktion benötigt Rose die eigene Position zur Fahrwegbestimmung. Dies erhöht den Grad der Koordinierung. Die Konfiguration ist je nach Algorithmus einfach bis komplex. Ein festmontierter Sensor auf einer mobilen Plattform benötigt eine einmalige Konfiguration. Externe Sensoren, zum Beispiel Kamerasysteme, benötigen eine größeren Konfigurationsaufwand. Da zunächst das Kamerasystem ausgewählt werden muss in dessen Arbeitsraum sich der gesuchte Agent befindet. Dadurch steigt auch der Koordinierungsaufwand.

\subsubsection{Funktion Übergabe}
\label{fun:han}
\begin{tabular}{|p{3cm}|p{10cm}|}
	\multicolumn{2}{c}{$f_5$ (Übergabe Objekt):}\\
	\hline  $Id$ & handover\\ 
	\hline  $I$ & $\emptyset$ \\ 
	\hline  $O$ & $\emptyset$ \\ 
	\hline  $\Phi$ & Bei Fehlschlag erfolgt Fehlermeldung.\\ 
	\hline $Pr$ & Aus Zustand $s_2$ möglich. Ein Objekt wurde gegriffen.  \\ 
	\hline $Po$ & Endet in Zustand $s_2$. Im Gegensatz zu $Pr$ befindet sich das Objekt im Greifer des anderen Roboters. Bei Fehlschlag endet die Funktion in $s_e$.\\ 
	\hline $Freq$ & Einmalig.\\ 
	\hline $Cost$ & Dynamisch. Abhängig von der Distanz zwischen den beiden Akteuren und der benötigten Bewegungen der Arme. \\
	\hline
\end{tabular} 

\paragraph{Beschreibung}
Diese Funktionalität bildet die zentrale Aufgabe dieser Arbeit ab und ist deshalb etwas . Ein Objekt soll zwischen zwei Robotern übergeben werden. Roboter A hat das Objekt gegriffen und soll nun dieses nur an Roboter B übergeben. Dazu müssen zunächst die beiden Arbeitsräume der Roboter eine Schnittmenge aufweisen in der das Objekt übergeben werden kann. Danach muss eine Übergabeposition bestimmt werden. Diese sollte möglichst Energieeffizient sein. Neben der Position spielt die Orientierung im Raum einen wichtigen Faktor, da je nach Objekt und Greifer die Griffpositionen an dem Objekt variiert. Da in dieser Arbeit mit einem einfachen symmetrischen Objekt, einem langgezogenen Quader, und einem Gripper mit parallelen Fingern gearbeitet wird, ist ein entgegengesetzter Griff möglich (siehe Abbildung \ref{fig:grip1}). Dabei entspricht die Pose von Greifer B der invertierten Pose von Greifer A:

\begin{equation}
\xi_{GreiferB} = \ominus \xi_{GreiferA}
\label{eq:grip}
\end{equation}


Neben der Greifposition ist auch die Pfadplanung zur Greifposition wichtig, da der sich nähernde Greifer weder mit dem Objekt noch mit dem anderen Greifer kollidieren darf. Ebenfalls kann die Symmetrie der Übergabe genutzt werden. Unter der Bedingung, dass die Finger von Greifer A nicht die selbe Position haben dürfen wie die Finger von Greifer B, muss die Gleichung \ref{eq:grip} um eine Translation $T_\tau$ entlang der X-Achse des Objekts erweitert werden. Dies ist in Abbildung \ref{fig:grip3} dargestellt. Die Länge der Translation $|T\tau|$ entspricht dabei mindestens der Hälfte der aufsummierten Breiten der Finger der Greifer.

\begin{equation}
\xi_{GreiferB} = \ominus \xi_{GreiferA} \oplus T_\tau
\label{eq:grip2}
\end{equation}

 \begin{figure}
 	\centering
  	\subfigure[Entgegengesetzter Griff: Die Ausrichtung der Greifer (grau) ist zueinander invertiert. Die Achsen liegen alle parallel, sind jedoch in gegensetzige ausgerichtet. Die Symetrie des Objekts (rosa) und die parallelen Finger der Greifer ermöglichen eine lineare Transjektion entlang der Z-Achsen (grün) ohne Kollisionen mit dem anderen Greifer oder dem Objekt.]{%
 		\includegraphics[scale=0.45]{fig/grip1}
 		\label{fig:grip1}}
 	\hfill
 	\subfigure[Translation($T_\tau$) an der X-Achse (blau) des Objektes (rose) zur Kollisionsvermeidung der Finger (grau). Die minimale Länge der Translation beträgt die Hälfter der aufsummierten Breite der Finger.]{%
 		\includegraphics[scale=0.45]{fig/grip3}
 		\label{fig:grip3}}
 	
 	\caption{Greifer bei der Übergabe}
 	\label{fig:grip}
 \end{figure}

Da diese Funktionalität ein MR-Task ist, werden Nachrichten zwischen den Agenten ausgetauscht, um den Ablauf zu koordinieren. Dieser Nachrichtenaustausch wird in den folgenden Sequenz-Diagrammen dargestellt und analysiert. Dabei werden die drei Arten der Übergabe Geben (Abbildung \ref{fig:gripper1}), Nehmen (Abbildung \ref{fig:gripper2}) und Rendezvous (Abbildung \ref{fig:gripper3}) auf die Anzahl der Nachrichten $k$, sowie deren Komplexität, untersucht. Ein weiterer Aspekt ist das zeitliche Verhalten $t$ und der Unterschied im Ablauf der drei Varianten.

 \begin{figure}
 	\centering
 	\subfigure[Dieses Sequenzdiagramm zeigt den Ablauf des Gebens. Die Aktion wird dabei vom objekttragenden Roboter (A) initialisiert.]{%
 		\includegraphics[scale=0.6]{fig/SeqGive}
 		\label{fig:gripper1}
	}
 	\hfill
 	\subfigure[In diesem Sequenzdiagramm wird der Ablauf des Nehmens abgebildet. Die Aktion wird dabei vom objektlosen Roboter initialisiert.]{%
 		\includegraphics[scale=0.6]{fig/SeqTake}
 		\label{fig:gripper2}
 	}
 	\hfill
 	\subfigure[Dieses Sequenzdiagramm zeigt die Rendezvous Variante der Übergabe. Die Aktion kann von beiden Agenten initialisiert.]{%
 		\includegraphics[scale=0.6]{fig/SeqRond}
 		\label{fig:gripper3}
 	}
 	\caption{Sequenzdiagramme für Übergabevarianten}
 	\label{fig:grips}
 \end{figure}

Die in Abbildung \ref{fig:gripper1} dargestellte Variante des \textit{Gebens} wird von dem Roboter initialisiert, der das Objekt trägt (Roboter A). Nach Bestimmung der Übergabe Position und Orientierung bewegt sich zunächst Roboter A in die Pose. Dabei ist eine lineare Annäherung nicht nötig, da noch kein Kollisionspotential besteht. Anschließend wird eine einfache Bestätigung an Roboter B geschickt, der sich anschließend in Bewegung setzt. Diese besteht zunächst aus einer Positionierung nahe des Objektes. Der Greifer ist dabei schon geöffnet und dem anderen Greifer entgegengesetzt orientiert wie in Abbildung \ref{fig:grip1}. Folgend kommt die Annäherung auf der linearen Trajektorie entlang der Z-Achse an das Objekt. Sobald der Greifer in Position ist wird dieser geschlossen und eine einfache Bestätigung an den anderen Roboter gesendet. Dieser öffnet seinen Greifer und bestätigt ebenfalls. Durch diese Nachricht löst sich nun Roboter B, ebenfalls auf der Z-Achse, um das Objekt aus dem Greifer von Roboter A zu entfernen. Darauf folgt eine Freigabe an Roboter A, welcher sich nun auch von der Position entfernt, und der Übergang in die sichere Candle-Stellung. Alle Nachrichten, die in dieser Variante ausgetauscht werden, sind einfache Bestätigungen oder Freigaben, die nur vom Zeitverhalten her dynamisch sind und keine weiteren Informationen beinhalten. Benötigt werden, ohne die Positionsbestimmung, vier Nachrichten zur vollständigen Koordinierung ($k_{Geben} = 4$). Für das Zeitverhalten gilt: $t_{Geben} = move_A + move_B + near_B + close_B + open_A + near_B + max(move_A, move_B)$ unter der Bedingung, dass $move_B \gg open_B$, da der Greifer von B während der Bewegung geöffnet wird. $move$ entspricht dabei der Zeit die für eine Bewegung von einer Pose in eine Zielpose benötigt wird, $near$ ist die Zeit für die positive oder negative Annäherung, $open$ und $close$ sind die benötigten Zeiten für das Öffnen und Schließen der Greifer.

In Abbildung \ref{fig:gripper2} ist das \textit{Nehmen} dargestellt. Im Gegensatz zum \textit{Geben} wird die Aktion vom dem Roboter initialisiert, an den das Objekt übergeben wird. Nach der Positionsbestimmung bewegt Roboter A den Greifer ohne Objekt an die Übergabeposition und öffnet diesen während der Bewegung. Anschließend fordert er das Objekt an. Darauf nähert sich Roboter B mit dem Objekt auf der linearen Trajektorie entlang der Z-Achse dem Greifer A an und führt den Gegenstand zwischen den Fingern von Greifer A ein. Nach einer Positionsbestätigung schließt Roboter A den Greifer. In diesem Zustand wird das Objekt von beiden Robotern fixiert. Nach der Schließung des Greifers schickt Roboter A eine Nachricht an Roboter B, welcher nun seinen Greifer öffnet und dies mit einer Nachricht bestätigt. Roboter A entfernt nun das Objekt aus dem Greifer von Roboter B und gibt die Position frei nachdem er den Kollisionsraum verlassen hat. Anschließend bewegen beide Roboter die Arme in die sichere Candle-Stellung.

Der Nachrichtenaufwand dieser Variante beträgt $k_{Nehmen} = 5$. Wie beim \textit{Geben}, ist die Komplexität der Nachrichten eher gering, da es sich nur um zeitkritische Freigaben handelt. Für das Zeitverhalten gilt, unter den gleichen Bedingungen und Angaben wie beim \textit{Geben}: $t_{Nehmen} = move_A + move_B + near_B + close_A + open_B + near_A + max(move_A, move_B)$.

Die Rendezvous-Übergabe in Abbildung \ref{fig:gripper3} wird die Initialisierungsbewegung nicht explizit von einem Roboter ausgeführt, sondern von beiden gleichzeitig. Dabei bewegen sich zunächst beide Arme auf die Übergabeposition, wobei sich beide Greifer das letzte Wegstück auf der linearen Trajektorie annähern. Beide Roboter bestätigen ihre Position, sobald sie diese erreicht haben. Anschließend schließt der Roboter A, ohne Objekt, den Greifer und bestätigt.Roboter B öffnet den Greifer und übergibt so das Objekt. Wie bei den anderen Varianten bewegt sich Roboter A nach einer Nachricht nun mit Objekt auf einer linearen Bahn von Greifer B weg. Nach der anschließenden Positionsfreigabe bewegen sich beide Roboter in die sichere Candle-Stellung.

Die Nachrichtenkomplexität ist bei dieser Variante etwas höher, als bei den anderen. Den Unterschied macht die erste Koordinierung, nachdem beide Arme in Position gegangen sind, da nicht definiert ist, welcher Roboter zuerst die Stellung erreicht und somit die Reihenfolge der Nachrichten nicht definiert ist. Die Komplexität des Inhaltes bleibt bei dieser Variante genauso gering, wie bei den anderen Variante, da es sich nur um Bestätigungen handelt. Die Anzahl der Nachrichten für diese Variante ist $k_{Rendezvous} = 5$. Für das Zeitverhalten gilt, unter den bekannten Bedingungen und Angaben:  $t_{Rendezvous} = max(move_A + near_A, move_B + near_B) + close_A + open_B + near_A + max(move_A, move_B)$.

Ein Vergleich der Kennzahlen der Varianten zeigt, dass der Koordinierungsaufwand beim \textit{Geben} am geringsten ist. Dies liegt vor allem an der Anzahl der Nachrichten: $k_{Geben} < k_{Nehmen} = k_{Rendezvous}$. Außerdem ist die Komplexität der Nachrichten untereinander identisch, mit der Ausnahme am Anfang der \textit{Rendezvous}-Variante. Für das Zeitverhalten gilt $t_{Rendezvous} < t_{Geben} = t_{Nehmen}$. Dies liegt vor allem an der Parallelisierung der Bewegung am Anfang der Aktion.

\paragraph{Einordnung}
Diese Funktion ist eng-gekoppelt und benötigt einen hohen Aufwand an Koordinierung. Neben der Übergabeposition und -orientierung müssen auch die Zeitvorgaben für das Öffnen und Schließen der Greifer, sowie die Bewegungen koordiniert werden. Dies macht diese Funktionalität zur komplexesten im ganzen System. Dies liegt vor allem daran, dass diese Funktion als MR-Task eingeordnet wird, da sie von zwei Agenten zusammen ausgeführt wird. Diese Agenten sollten Multi-Task fähig sein, da einige Subtasks parallel ausgeführt werden sollen, zum Beispiel das Öffnen des Greifers, während der Arm in eine Position fährt. Ebenfalls komplex ist die Positionsbestimmung der Übergabe. Zunächst müssen dabei die beiden Arbeitsräume einen Schnittraum aufweisen. Eine notwendige Positionskorrektur kann dabei nur vom mobilen Roboter ausgeführt werden. Dies muss unter anderem in der Konfiguration berücksichtigt werden. Dadurch steigt auch der Konfigurationsaufwand für die Aufgabe, da sichergestellt werden muss, dass die Arbeitsräume der beiden Roboter sich überhaupt schneiden können. Weitere Aspekte der Konfiguration sind die Kompatibilität der Greifer zum Objekt und die Erreichbarkeit der nachfolgenden Ziele.


\subsection{Agenten}
\label{sec:agent}
Aus den Nicht-Funktionalen Anforderungen und den Funktionen lassen sich die folgenden Agenten im MRS extrahieren. Diese Agenten bestehen aus den Sensoren aus Kapitel \ref{sec:aufbau-sensoren} und den Robotern Rose und Dummy. Für die Roboter steht die Zuweisung der Funktionen und Rollen im System fest, die anderen Aktoren und Sensoren werden den Funktionen zugewiesen. Für Funktionen bei denen unterschiedliche Sensoren oder Aktoren zur Verfügung stehen wird evaluiert welcher Hardware die Funktion am besten löst.

\subsubsection{Raumüberwachung}
Die Raumüberwachung ist zur Identifizierung und Lokalisierung von Objekten zuständig (siehe $f_2$\ref{sec:funraum}). Dabei soll das Kamerasystem eine große Fläche mit einer hohen Auflösung observieren. Als Hardware Lösungen standen dabei das XTion Kamera System oder der Argos3D Sensor zur Auswahl. Da der Tiefensensor jedoch eine kleinere Auflösung als die XTion hat und kein Farbbild liefert, wird für die Umsetzung des MRS die XTion ausgewählt. Diese wurde plangemäß nach Abbildung \ref{fig:basic-aufbau-teststand} und \ref{fig:basic-aufbau-teststandh} an der Wand montiert. Zur Befestigung wurde eine zusätzliche Halterung mit dem 3D-Drucker gedruckt. Diese ermöglicht einen größeren Winkel in der Ausrichtung. Der eingestellte Winkel beträgt -62 \textdegree um die globale X-Achse. Durch die Höhe und den eingestellten Winkel ergibt sich eine Observerationsfläche auf der globalen X-Y-Ebene von 5,075 $m^2$. In Abbildung \ref{fig:imghawk} sieht man eine aufgenommene Punktwolke der angebrachten Kamera. Auf dieser ist die Trapezform der Punktwolke sehr gut sichtbar. Diese ergibt sich aus der Position und Orientierung der Kamera. Bedingt dadurch weisen nähere Objekte eine größere Punktdichte auf als entfernte Objekte. Auf der Abbildung sind zwei Bereiche ohne Punkte markiert. Die blaue Markierung zeigt auf eine Verschattung durch den Roboterarm. Der grüne Bereich weist viele schlecht gematchte Punkte zwischen Farb- und IR-Bild auf. Dies liegt vor allem an den schlecht reflektierten IR-Punkten auf dem Heizkörpergitter.

\begin{figure}
	\centering
	\includegraphics[scale=1.0]{fig/imghawk}
	\caption[Raumüberwachung Aufnahme]{Aufnahme der XTion als Raumüberwachung. Blaue Markierung: Toter Winkel der IR Aufnahme. Grüne Markierung: Heizkörper mit diffusen IR-Reflektionen.}
	\label{fig:imghawk}
\end{figure}

\subsubsection{Nahfelderkennung}
Die Nahfelderkennung setzt die Funktionalität $f_3$ (siehe \ref{sec:funnah}) um. Dabei wird der Sensor auf dem mobilen Roboter befestigt, um ein Objekt auch im Nahfeld identifizieren und lokalisieren zu können. Die Lokalisierung soll dabei wesentlich genau sein, als bei der Raumüberwachung. Zur Auswahl standen dabei wieder die XTion und der Argos3D Sensor. Der Vorteil des Sensors ist die geringe minimale Reichweite. Diese funktioniert schon ab 100 mm, während die XTion einen mindest Abstand von 800 mm braucht. So ist eine Montage des Argos Sensors am Greifer direkt möglich, während die XTion mit Abstand montiert werden muss. Nachteil ist die zusätzliche Stromversorgung, die der Sensor benötigt, und der fehlende Farbkanal. Dennoch wurde der erste Prototyp mit dem Argos3D Sensor entwickelt, da er direkt am Greifer montiert werden kann. Ziel des Prototypen war es die Punktwolke eines Objektes von dem planaren Hintergrund zu separieren. Der Prototyp nutzt dafür den RANSAC-Algorithmus mit einem Flächenmodell. Jedoch ergab sich schnell, dass dieser Ansatz nicht mit dem Sensor funktioniert. Die Abbildung \ref{fig:argos} zeigt drei Aufnahmen einer Punktwolke die mit dem Sensor aufgenommen wurden. Die Aufnahmen entstanden von einem Objekt, dass auf einem Tisch liegt. In der Abbildung \ref{fig:argostop} ist die Draufsicht auf die Punktwolke dargestellt. Dabei stellen die unterschiedlichen Farben die Intensität der gemessenen IR-Werte. Das Objekt ist auf dieser Ansicht nur durch die Gradienten der Intensität an den Konturen zu erkennen. Die Abbildung \ref{fig:argoside} ist eine Seitenansicht der selben Punktwolke. Das Objekt ist an den fehlenden Punkten in der Mitte der Punktwolke erkennbar. Diese Punkte sind nicht sichtbar, da die Punktwolke konvex gewölbt ist. Punkte die näher an der Kamera sind verschwinden somit aus dieser Perspektive hinter der restlichen Punktwolke. Die konvexe Wölbung der Punktwolke ist das Problem der Separierung mit einem planaren Modell. Damit das planare Modell auf die Punktwolke angewandt werden kann muss die Fehlerschranke des Algorithmus sehr hoch gesetzt werden. Dadurch werden aber auch die Punkte des Objektes in die Fläche eingeschlossen. Eine Alternative zum planaren Modell wäre die Anwendung eines sphärischen Modells. Dabei muss die Sphäre aber geschlossen sein und nicht nur einen Teilabschnitt der Oberfläche sein.  Die Abbildung \ref{fig:argospowside} zeigt nochmal die Seitenansicht. Diesmal wurde jedoch der Sensor mit einer zusätzlichen externen Stromquelle versehen. Dieses ergab immer noch eine gekrümmte Oberfläche, welche jedoch schwächer ausgeprägt war als vorher. Ein Versuch die Krümmung aus der Punktwolke herauszurechnen, ähnlich wie bei einer Fischaugenlinse, endete in ziemlichen Abweichungen der Punkte, da die Entfernung zur Oberfläche und die Linsenkrümmung unbekannt ist.

\begin{figure}
	\centering
	\subfigure[Topansicht der Punktwolke, die Farben repräsentieren die Intensität der zurückgeworfenen IR-Strahlen. In der Mitte sind die Umrisse des Objektes erkennbar. Am Rand sind einige abweichende Punkte zu sehen.]{%
		\includegraphics[scale=0.3]{fig/argostop}
		\label{fig:argostop}}
	\hfill
	\subfigure[Seitenansicht der Punktwolke. Sichtbar sind in der Mitte das Objekt und die kubische Grundform der Punktwolke, die eigentlich der planaren Ebene entsprechen sollte.]{%
		\includegraphics[scale=0.3]{fig/argoside}
		\label{fig:argoside}}
	\hfill
	\subfigure[Seitenansicht der Punktwolke mit zusätzlicher Stromversorgung am Sensor. Sichtbar ist wieder das Objekt in der Mitte. Die Krümmung der Oberfläche ist schwächer als bei Abbildung \ref{fig:argoside}]{%
		\includegraphics[scale=0.3]{fig/argospowside}
		\label{fig:argospowside}}
	\caption[Argos3D Punktwolke]{Aufgenommene Punktwolke mit dem Argos3D Sensor. Abgebildet ist ein quaderförmiges Objekt auf einem Tisch.}
	\label{fig:argos}
\end{figure}

Nach den negativen Tests mit dem Sensor folgten Überlegungen, ob man die XTion Kamera für die Naherkennung einsetzen kann. Problem war die große minimal Distanz für die Erkennung. Damit die benötigten 800 mm zum Boden erreicht werden, ist die Kamera am Greifer montiert, siehe Abbildung \ref{fig:xtionarm} Befindet sich der Arm in der Candle-Pose ist die Kamera 800 mm vom Untergrund entfernt und erfasst das Feld direkt vor der mobilen Plattform. TODO Infos. Durch eine Drehung des Armes, um das erste oder fünfte Gelenk, kann das Sichtfeld der Kamera erweitert werden. Jedoch gab es auch bei den ersten Tests mit der Kamera einige Probleme. Durch den eingebauten Weißabgleich wurde die Helligkeit des Bildes bei einer zu starken Einstrahlung reduziert. Das führte im Labor unter anderem zu einem schwarzen Bild und resultierte aus der Lichtreflexion der Aluminiumplatte die auf der mobilen Plattform montiert ist. Der Fehler konnte durch Abkleben der reflektierenden Oberfläche beseitigt werden.

\begin{figure}
	\centering
	\subfigure[Die XTion Kamera(rot) am Greifer montiert. Das Sichtfeld liegt vor der mobilen Plattform. Durch eine Drehung des Armes kann das Sichtfeld um den Roboter erweitert werden.]{%
		\includegraphics[scale=0.8]{fig/xtionarm}
		\label{fig:xtionarm}}
	\hfill
	\subfigure[TODO]{%
		\includegraphics[scale=0.3]{fig/xtionarm}
		\label{fig:xtionarmbild}}
	\caption[XTion als Naherkennung]{Die Asus XTion als Hardware Lösung für die Naherkennung}
	\label{fig:xtionnah}
\end{figure}


\subsubsection{Lokalisierung}
\label{seq:lok}
Für die Lokalisierung, $f_4$ in Kapitel \ref{sec:funl}, sind alle Sensoren potenziell möglich. Die einfachste Variante wäre die Auswertung der Odometriedaten. Dazu wird die Geschwindigkeit der Plattform mit der Laufzeit multipliziert um den zurückgelegten Weg zu berechnen. Um die Genauigkeit zu bestimmen wurden im Rahmen dieser Arbeit Versuche durchgeführt. Dazu wurden zunächst linearen Bewegungen entlang einer Achse ausgeführt und gemessen. Anschließend wurden kombinierte lineare Bewegungen entlang der X- und Y-Achse getestet. Die dabei verwendeten Geschwindigkeiten und Laufzeiten wurden variiert. Gemessen wurde anschließend die zurückgelegte Distanz bei zehn Versuchen pro Messgruppe. Für die anschließende Auswertungen wurden die arithmetischen Mittelwerte für die absolute und relative Abweichung über den Versuchsgruppen berechnet. Die anschließende Bewertung wurde für die einzelnen Versuchsgruppen im Bezug aufeinander und der Geschwindigkeit, sowie Laufzeit vorgenommen. Dabei ergab sich, dass es eine starke positive Kovarianz zwischen Geschwindigkeit und Abweichung gibt. Diese liegt vor allem an den nicht sehr genauen Motoren-Controllern und dem Anhalten, da die Plattform nicht langsam negativ beschleunigt, sondern die Geschwindigkeit direkt auf null setzt. Auch die Kovarianz zwischen Laufzeit und Abweichung ist positiv ausgeprägt, erreicht aber nicht den Wert der Geschwindigkeit. Die größten relativen Abweichungen zeigen die kombinierten Bewegungen bei hohen Geschwindigkeiten. Auch die seitlichen Bewegungen auf der Y-Achse der Plattform zeigen starke Abweichungen. Diese sind auf die omnidirektionalen Räder zurückzuführen. Auffällig sind auch die hohen relativen Abweichungen bei kurzen Laufzeiten. Nach Beobachtung der mobilen Plattform lässt sich dies mit dem Schlupf der Räder und dem glatten staubigen Untergrund erklären. So scheinen die Räder zunächst durchzudrehen. Ein weiteres Problem scheint die Ansteuerung zu sein. Das Testprogramm zur Versuchssteuerung schickt zunächst asynchron das Signal mit den Geschwindigkeiten an die Motorensteuerung. Anschließend pausiert die Steuerung für die eingestellte Laufzeit, bevor das Stopp-Signal asynchron an die Motoren geschickt wird. Dabei entsteht eine unbekannte Verzögerung durch das asynchrone Steuersignal. Zusammengefasst ist diese Methodik alleine zur Lokalisierung des Roboters ungeeignet, da die Abweichungen zu groß sind. Eine mögliche Fehlerkorrektur ist auf Grund des hohen Standardfehlers und den zufälligen Umweltfaktoren (Schlupf und Signalverzögerung) nur schwer realisierbar.

Eine weitere Option zur Lokalisierung ist die visuelle Odometrie. Dabei wird der optische Fluss einer Bildsequenz genutzt um die eigene Geschwindigkeit und Positionsveränderung zu bestimmen. Dazu müssen zunächst in zeitlich aufeinanderfolgenden Bildern Features detektiert und untereinander zugewiesen werden. Durch die Veränderung der Position auf dem Bild aller Features entsteht ein Vektorfeld. Dieses wird zur Bewegungsschätzung der Kamera genutzt. Diese Methodik benötigt jedoch ein Kamerasystem mit einer geringen Verzögerungszeit. Das XTion Kamerasystem hat an dem Rechner von Dummy eine Verzögerung von einer Sekunde. An dem eingebauten Rechner in der mobilen Plattform beträgt die Verzögerung über drei Sekunden und die Bildrate unter ein Bild pro Sekunde, was für eine zuverlässige Ortung nicht reicht.

Auf der Entwicklungsseite des YouBots wird der Lasersensor Hokuyo URG-04LX-UG01 für die Navigation angeboten. Auch Peter Corke nutzt in \cite[Kapitel 6]{Corke2011} einen Hokuyo Lasersensor zur Navigation. Aus Prioritäts- und Umfangsgründen wird in dieser Arbeit mit einem Hybridverfahren zwischen Karten- und Odometriebasierter Lokalisierung gearbeitet. Dazu wird eine grobe Position mit Hilfe der Odometrie errechnet. Diese wird in regelmäßigen Abstände mit den Daten des Lasersensors korrigiert, um einen möglichen Schleppfehler zu verhindern.

\subsection{Architektur RATS}
In diesem Kapitel wird die Software-Architektur \textit{RATS} vorgestellt. RATS steht für \textit{Roboter Action and Task System} und setzt die Anforderungen für dieses MRS um. Dabei liegt ein besonderes Augenmerk auf der einfachen Erweiterbarkeit und Konfiguration des ganzen Systems. So können neue Aktoren und Sensoren ohne großen Aufwand und Veränderung des bestehenden Systems eingefügt werden. Im Folgenden werden das Konzept und die einzelnen Bestandteile der Architektur erklärt. Da das System auf ROS aufbaut wurde schon in der Architektur dem Package und Node Konzept gefolgt.

\subsubsection{Konzept}
Das Klassendiagramm in Abbildung \ref{fig:classrats} bildet die Architektur von RATS ab. Das Motiv hinter der Architektur ist die Zerlegung der Aufgaben in Actions und Tasks für Agenten in MRS. Jede Action entspricht dabei einer Funktion die ein Agent anbieten kann. Jede Task ist eine Abfolge von Actions, welche parallel oder seriell vom Agenten ausgeführt werden, und ist ebenfalls eine Action. Dadurch können Tasks eigene Subtasks ausführen. Jede Action und Task besitzt eine ID, durch diese ID kann einem Agenten mitgeteilt werden, welche Action oder Task er ausführen soll. Dieser Befehl kann von dem zentralen RATSCore oder jedem anderen Agenten im System ausgerufen werden. Die Agenten werden in dieser Architektur durch die RATSMember abgebildet. Jeder RATSMember besitzt eine Menge an Actions und Tasks, die von dem Agenten ausgeführt werden. Diese Menge an Actions und Tasks werden bei der Initialisierung an den RATSCore übertragen. Dieser übernimmt die Koordinierung und Konfigurierung im System.

\begin{figure}
	\centering
	\includegraphics[scale=0.5]{fig/classrats}
	\caption[Klassenübersicht RATS]{Klassendiagramm RATS in der Übersicht. Die zentrale Komponente ist das RATS Paket. Dieses definiert die Schnittstellen Action, Task und RATSMember. Der MRS Controller RATSCore gehört nicht zum RATS Paket, da er je nach Konfigurations- und Koordinierungsaufwand unterschiedlich implementiert werden kann. Das Paket RATSYoubot implementiert die RATSMember von Rosie und Dummy. Alle anderen Sensoren und Aktoren implementieren die RATSMember Schnittstelle und sind jeweils in eigenen Paketen organisiert.}
	\label{fig:classrats}
\end{figure}


Als Grundlage für die Architektur liegt das Strategy Entwicklungsmuster. Dabei stellt die Action, beziehungsweise der Task, die Strategy und die RATSMember den Kontext dar. Soll eine Action ausgeführt werden wird, lädt der RATSMember die Action. Dazu wird zunächst der aktuelle Kontext an die Action übergeben. Dieser Kontext beinhaltet mögliche Parameter die benötigt werden. Dabei wird zwischen lokalen und globalen Kontext unterschieden. Der lokale ist nur während der Ausführung der Task oder Action verfügbar und wird anschließend gelöscht. Der globale Kontext ist Action und Agent übergreifend. Er kann dazu genutzt werden Umweltinformationen abzubilden und Allgemein verfügbar zu machen. Dieses Konzept ist dem PEIS-Konzept mit seinem Tupelförmigen Speicher angeglichen (siehe Kapitel \ref{sec:peis-imple}). Nach der Zuweisung des Kontext wird die \lstinline|execute()|-Methode der Action aufgerufen. Diese führt die gewünschte Funktionalität aus. Dabei kann der Kontext geändert werden. Dies ist vor allem bei Sensoren nötig, da diese ihre Daten nicht über einen Rückgabewert an den Aufrufer zurückgeben, sondern die Daten in den globalen Kontext schreiben. Ein Eingabeparameter dient dabei als Pfad für den Speicher. Ein Beispiel ist in Abbildung \ref{fig:seqspeicher} dargestellt. Dieses zeigt ein Sequenzdiagramm zur Anwendung des globalen Speichers. Leere Antworten wurden zur Übersichtlichkeit ausgelassen. Der Anwender setzt einen Befehl an den Core. Dieser Befehl beinhaltet einen Farbwert-Parameter, der der Oberflächenfarbe eines Objektes entspricht. Dieser Parameter wird unter einem Objekt mit der ID \textit{objid} abgelegt. Anschließend wird ein Sensor aktiviert, der das Objekt finden soll. Bei der Aktivierung wird der Objektpfad für den Speicher übergeben. Der Sensor holt sich die Daten aus dem globalen Speicher, lokalisiert das Objekt und schreibt die Position in den Speicher unter den selben Objektpfad zurück. Im Speicher liegen nun die Daten zu Farbe und Position des Objektes. Nach der Lokalisierung wird an einen Aktor der Befehl zur Aufnahme gesendet. Als Parameter wird nur die Objektid übergeben. Der Aktor lädt die zuvor gesammelten Daten aus dem Speicher und hebt das Objekt an der übermittelten Position auf.

\begin{figure}
	\centering
	\includegraphics[scale=1.0]{fig/seqspeicher}
	\caption[RATS globaler Speicher]{Beispielhaftes Sequenzdiagramm zur Erklärung vom globalen Speichersystem. Zur Vereinfachung ist ein Erfolgsmeldung dargestellt und leere Antworten wurden entfernt.}
	\label{fig:seqspeicher}
\end{figure}

\subsubsection{RATSAction und RATSTask}
RATSAction und RATSTask bilden die Funktionalitäten innerhalb der Architektur ab. Wie im Konzept beschrieben werden sie von außen angestoßen und durch den Agenten eingesetzt. Damit die Aktion oder der Task erreichbar ist, kann sie über eine ID erreicht werden. Die ID steht dabei für eine Funktionalität des MRS. Eine Funktionalität kann von verschiedenen Agenten unterschiedlich implementiert werden. Sie wird dann als Action oder Task mit der dazugehörigen ID publiziert. Dieses Konzept folgt der Definition aus Kapitel \ref{sec:dev-funk}. Außerdem kann jede Action, einschließlich Tasks, den aktuellen Zustand des Systems ändern. Dies geschieht über den globalen Speicher. 

Die zentrale Methode jeder Action ist  \lstinline|execute()|-Methode. Diese besitzt die Logik der Action und ist immer gleich aufgebaut. Zuerst wird der lokale Kontext ausgelesen. In diesem befinden sich mögliche Zeiger auf den globalen Speicher. Dieser wird darauf ausgelesen. Darauf folgt eine Überprüfung, ob alle benötigten Eingabeparameter vorhanden und gültig sind. Dies betrifft auch den aktuellen Systemzustand. Ist diese Überprüfung ungültig bricht die Funktion mit einer Fehlermeldung ab. Anschließend folgt die Ausführung der Funktion. Auch hier wird bei einer Fehlfunktion eine Rückmeldung gegeben und die weitere Ausführung abgebrochen. Zum Abschluss schreibt die Action zunächst Ergebnisse in den globalen und dann in den lokalen Kontext. Neben der Ausführung besitzt jede Action eine Methode die Kosten für eine Funktion vor zu berechnen. Dabei ist die Kostenfunktion von der Funktion und den Umweltparametern abhängig. Diese Methode wurde für eine automatische Konfiguration vorgesehen. So werden die Actions mit ihrer Funktion-ID und den Umweltparametern, im globalen Speicher, angeboten. Alle Agenten, die eine Action mit dieser ID haben, können nun die Kosten ermitteln und mit diesen bieten. Dabei gewinnt der Agent mit den geringsten Kosten. Die Kostenfunktion greift nur lesend auf den Speicher zu und führt die Aktion auch nicht aus.

RATSTasks bieten die Möglichkeit Actions miteinander zu verbinden. Dabei sind die Tasks selber Actions und können so ineinander verkettet werden. Der einfachste Task ist eine serielle Ausführung der hinterlegten Actions. Bricht eine Action ab, bricht der ganze Task ab. Komplexere Tasks steuern parallele oder bedingte Ausführungen von Actions. Dabei können Fehler abgefangen und behandelt werden. So ermöglicht ein Task, dass ein Roboterarm nach einem Fehlschlag der Action in einen sicheren Zustand (Candle- oder Fold-Pose) wechselt.


\subsubsection{RATSMember}
\label{sec:devmember}
Die RATSMember repräsentieren die Agenten im MRS. Jeder RATSMember registriert sich bei der zentralen Komponente, dem RATSCore, mit einer eindeutigen ID. Ist diese ID im System schon vergeben, beendet sich der Agent und es muss eine neue ID vorgegeben werden. Unter dieser ID kann der Agent im MRS erreicht werden. Neben der ID besitzt jeder RATSMember eine Menge an Tasks und Actions die von diesem ausgeführt werden können. Diese Menge übersendet der RATSMember bei der Registrierung an den RATSCore. 

Da ROS als Grundlage für diese Arbeit dient, tritt jeder RATSMember als selbstständiger ROS-Node auf. Dies dient vor allem zur Stabilisierung und Wiederherstellbarkeit des Systems. Die ROS Node-ID von einem RATSMember-Node wird auch als ID des RATSMember genutzt. Dadurch kann innerhalb eines MRS die Eindeutigkeit der ID gewährleistet werden. Damit ein RATSMember eine Action ausführt wird, muss der Member eine Nachricht mit der Action-ID und den Parametern erhalten. Zunächst war für diese Kommunikation ein ROS-Topic angedacht, wie im PEIS Konzept vorgesehen (siehe Kapitel \ref{sec:peis-imple}). Nach der Entwicklung eines ersten Prototypen zeigte sich jedoch, dass diese Methodik Nachteile bringt. So kann ein Publisher an einem ROS-Topic nicht sicherstellen, ob ein Subscriber die Nachricht erhalten hat. Außerdem ermöglichen Topics keine Rückgabewerte. Deshalb wurde für die weitere Entwicklung die Action-Lib von ROS (siehe \ref{sec:basic-ros-action}) genutzt. Diese ermöglicht einen RPC ähnlichen Aufruf. Gegenüber den Services von ROS können außerdem Zwischenergebnisse zurückgegeben werden. Dies ermöglicht eine Schnittstelle für komplexere MR-Tasks mit hohem Koordinierungsaufwand. Ein Problem stellen die dynamischen Parameterlisten dar. Diese sind für das System nötig, da je nach Koordinierungsaufwand unterschiedlich viele Daten mit variablem Datentypen übergeben werden muss. Die Action-Lib, wie auch Services und Topics, lassen keine dynamischen Parameterlisten innerhalb ihrer Messages zu. Darum werden diese Nachrichtenparameter als Strings definiert. Der Inhalt dieser Strings sind JSON-Objekte. Der Aufbau der JSON Objekte ist in Abbildung \ref{fig:jsonmsg} dargestellt. Das Root-Element besteht aus dem Objekt Header und dem Array der dynamischen Parameterliste. Der Header wird nur in der Kommunikationsschicht ausgewertet und beinhaltet die Agent- und die Action-ID. Die Parameterliste wird vom RATSMember geparst und dem lokalen Kontext hinzugefügt. Nach Ausführung der RATSAction wird aus dem lokalen Kontext ein JSON-Objekt erstellt, welches anschließend als Rückgabewert in der Message gesendet wird. Sollte eine RATSAction fehlschlagen wird in den Header ein Fehlercode eingetragen, der vom Aufrufer ausgewertet wird. Als JSON-API kommt die RapidJSON Bibliothek zum Einsatz.

\begin{figure}
	\centering
	\includegraphics[scale=1.0]{fig/jsonmsg}
	\caption[JSON Action Message]{Aufbau einer JSON Action Message. Unter dem Root-Element hängen der Header und die Parameter-Liste. Der Header beinhalten Informationen über die Agent-ID und die Action-ID. Die Parameter-Liste enthält den dynamischen lokalen Kontext.}
	\label{fig:jsonmsg}
\end{figure}

\subsubsection{RATSCore}
Der RATSCore ist der zentrale Koordinierungs- und Konfigurationscontroller. An diesem melden sich alle RATSMember an und ab. Des Weiteren bietet der RATSCore Schnittstellen für Steuerungsoberflächen. Für diese Arbeit wird eine einfache Konsole umgesetzt, aus der die gewünschten Actions und Tasks  gestartet werden können. Es sind aber auch eine andere Ansteuerung, wie Sprach- oder Gestensteuerung realisierbar. Die Konfigurations- und Koordinierungsaufgaben werden in dieser Arbeit mit Hilfe von Textdateien vorgegeben. Dabei wird die Agenten-ID, die Action-ID und die nötigen Parameter, zum Beispiel Zeiger auf den globalen Speicher, angegeben. Diese Textdateien werden von dem Core zur Laufzeit eingelesen. Der Core übernimmt dann die Ansteuerung und Verwaltung der RATSMember.

Der RATSCore ist ein selbstständiger ROS-Node, der mit dem ROS Core zusammen gestartet wird. Da er eine zentrale Komponente darstellt ist er ein potenzieller Single-Point-of-Failure. Fällt er aus bricht das System zusammen. Je nach Systemzustand kann dies zu weiteren Problemen führen. Damit dies abgesichert ist, sind Sicherheitsmechanismen in die unterschiedlichen Kommunikationsschichten eingebaut. So brechen RATSMember ihre Aktionen im Ausfall nicht ab, sondern führen diese bis zum Ende aus. Können sie dann keine Rückmeldung an den RATSCore senden gehen die RATSMember in einen sicheren Zustand über. Dies betrifft besonders mechanische Aktoren, wie die Roboterarme. Bekommt der Core keine Rückmeldung mehr von einem RATSMember nach einer bestimmten Zeit, so bricht er die gesamte Aufgabe ab. Meldet sich ein RATSMember im Ruhezustand ab, werden seine Actions und Tasks aus dem Speicher genommen, so wird er bei zukünftigen Aufgaben nicht mit eingeplant. Dadurch sichert die RATSCore einen sicheren Systemzustand. Ist ein RATSCore einmal abgestürzt müssen auch alle Member einmal neu gestartet werden. Dadurch registrieren sie sich neu an dem RATSCore.

\subsection{Inverse Kinematik}
In diesem Kapitel wird die Entwicklung der inversen Kinematik vorgestellt. Dabei wird zunächst auf die schon existierenden inversen Kinematiken für den YouBot Arm eingegangen und eine Bewertung dieser vorgenommen. Darauf folgt die Entwicklung eines stark vereinfachten Prototypens, dessen Weiterentwicklung die endgültige inverse Kinematik darstellt. Den Abschluss bildet die Optimierung der Lösungen der inversen Kinematik mit Bezug auf die Pfadfindung.

\subsubsection{YouBot Kinematiken}
Mit dem YouBot wird bereits das Software-Paket \textit{MoveIt!} zur Bewegungs- und Pfadbestimmung mitgeliefert. Es ist ROS-basiert und nutzt zur Visualisierung und Konfiguration ein RViz-Plugin. Die implementierte Pfadbestimmung verwendet numerische Ansätze für das Lösen der Bewegungsbahnen. Dabei stehen folgende Planungsbibliotheken zur Auswahl: Die \textit{Open Motion Planning Library} (OMPL), \textit{Covariant Hamiltonian Optimization and Motion Planning} (CHOMP) und \textit{Search-based planning} (SBPL). Die OMPL beinhaltet verschiedene Algorithmen zur Bewegungsplanung die alle auf abstrakten Zuständen des Systems beruhen. Dabei wird zwischen probabilistischen (PRM) und Baum-basierten Algorithmen (RRT, EST, KPiece, SBL) unterschieden. Alle Algorithmen lassen sich konfigurieren, dies betrifft vor allem die Anzahl der Iterationen und die Zeitschranke für die Dauer der Berechnung. Für diese Arbeit wurden drei verschiedene Algorithmen (PRM, EST, CHOMP) für eine Bewertung ausgewählt und evaluiert. Bei dieser Evaluierung wurden fünf Bewegungsziele abgesteckt, welche vom Roboter angefahren werden sollten. Die Anzahl der Iterationen wurden in drei Messgruppen (zehn, 100 und 1000 Iterationen) aufgeteilt und mit jedem Algorithmus getestet. Die Zeitschranke wurde auf 60 Sekunden eingestellt, um möglichst optimale Ergebnisse zu erreichen. Pro Bewegungsziel wurde die Dauer der Planung $t$ in Sekunden und die Genauigkeit der Endpose in Zentimetern und Grad gemessen. Dadurch setzt sich ein Bewertungsvektor $x \in  \mathbb{R}^3$ zusammen:

\begin{math}
	x = \left(\begin{array}{c} t \\ \chi \\ \phi \end{array}\right)	
\end{math}

Dabei wurden die euklidische Distanz für den linearen und den rotierenden Anteil der Pose ermittelt. Wobei die $\lambda$-Funktion den linearen und die $\theta$-Funktion den rotierenden Anteil extrahiert: 

\begin{math}
	\chi = \|\lambda(\xi_{Ziel}) - \lambda(\xi_{Plan})\|_2 = \sqrt{\sum_{i=1}^{3}(\xi_{Ziel,i}-\xi_{Plan,i})^2}	
\end{math}

\begin{math}
	\phi = \|\theta(\xi_{Ziel}) - \theta(\xi_{Plan})\|_2 = \sqrt{\sum_{i=4}^{6}(\xi_{Ziel,i}-\xi_{Plan,i})^2}
\end{math}

Anschließend wurden die Bewertungsvektoren der fünf Bewegungsziele elementweise quadriert aufsummiert.

\begin{math}
	x_{all} = \left(\begin{array}{c}
				\sqrt{\sum_{i=1}^{5} t_i^2}/5\\ 
				\sqrt{\sum_{i=1}^{5} \chi_i^2}/5 \\
				\sqrt{\sum_{i=1}^{5} \phi_i^2}/5 \end{array}\right)	
\end{math}


Die Ergebnisse pro Iterationen-Messgruppe und Algorithmus sind in Tabelle \ref{tab:mesplan} aufgelistet. In der Auswertung fallen bestimmte Charakteristiken der einzelnen Algorithmen auf. So benötigt EST schon bei wenigen Iterationen mehr Zeit als die anderen Algorithmen. Dafür sind die Abweichungen geringer. PRM und CHOMP sind zeitlich betrachtet sehr ähnlich. Differenzen zeigen sich erst bei der Betrachtung der linearen und rotierenden Abweichungen. So sind die linearen Abweichungen bei PRM immer kleiner als bei den Planungen mit CHOMP. Für die Rotation gilt dies genau umgekehrt. Dort ist CHOMP genauer als PRM. Zusammengefasst sind die Ergebnisse für das MRS dieser Arbeit jedoch ungenügend. Eine Genauigkeit von 1,5 cm linear oder 0,8\textdegree rotierend  ist für die Anwendung nicht ausreichend. Auch die benötigte Zeit der Planungen ist zu hoch. Ein Grund für diese ist die schlechte Hardware des eingebauten Rechners. Vergleichstests auf Dummy zeigten fünf mal schnellere Ergebnisse.

\begin{table}
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline  	& EST & PRM & CHOMP \\ 
		\hline 10 	& 
		$\left(\begin{array}{c} 1.84 \\ 6.253 \\ 11.6  \end{array}\right)$ & 
		$\left(\begin{array}{c} 0.32 \\ 10.486 \\ 17.4  \end{array}\right)$ &
		$\left(\begin{array}{c} 0.46 \\ 11.731 \\ 13.2  \end{array}\right)$ \\ 
		\hline 100 	&
		$\left(\begin{array}{c} 15.84 \\ 2.214 \\ 2.1  \end{array}\right)$ & 
		$\left(\begin{array}{c} 3.84 \\ 3.164 \\ 4.6  \end{array}\right)$ &
		$\left(\begin{array}{c} 5.21 \\ 4.756 \\ 2.9  \end{array}\right)$ \\ 
		\hline 1000 &  
		OOT &
		$\left(\begin{array}{c} 36.15 \\ 1.54 \\ 1.2  \end{array}\right)$ &
		$\left(\begin{array}{c} 40.84 \\ 1.78 \\ 0.8  \end{array}\right)$ \\ 
		\hline 
	\end{tabular}
	
	\caption[Gemittelte Messwerte der MoveIt!-Planer-Algorithmen]{Die gemittelten Messwerte der MoveIt!-Planer. In den Zeilen sind die unterschiedlichen Iterationen pro Planung gegeben. Die Spalten geben den verwendeten Planer-Algorithmus an. Die Messwerte $x$ setzen sich aus der benötigten Planungszeit $x_1$ in Sekunden und der euklidischen Distanz der Zielpose zur Planpose zusammen. Dabei wird zwischen linearer $x_2$ (Zentimeter) und rotierender $x_3$ (Grad) Distanz unterschieden. OOT steht für eine Überschreitung der Zeitschranke bei allen Bewegungszielen.}
	
	\label{tab:mesplan}
\end{table}
	
Eine weitere inverse Kinematik, beziehungsweise Bewegungsplaner, wurde 2013 von der Robotics \& Perception Group der Universität Zürich veröffentlicht (siehe \cite{muggler2013torque}). Dieser Planer unterscheidet sich von dem Kuka Ansatz in der Ansteuerung der Hardware. Dabei wird nicht nur die Konfiguration der Gelenke, sondern auch die  Drehkraft der Gelenke angesteuert. Dadurch können Bewegungen entlang einer Trajektorie ausgeführt werden. Der Planer basiert ebenso auf der MoveIt!-Basis und nutzt die implementierten Planungsalgorithmen. Für diese Arbeit konnte dieser Bewegungsplaner nicht eingesetzt werden, da die Bibliothek zu dem Zeitpunkt der Entwicklung nicht die installierte Firmware der YouBots unterstützte. Inzwischen sind die benötigten Updates installiert und könnten genutzt werden. Diese Änderung würde jedoch den Aufwand der Arbeit überschreiten. Die inverse Kinematik die dabei genutzt wird ist ein Nachfolger der ersten Entwicklung von Kuka. Diese beruht auf den geometrischen Eigenschaften des Arms und ist in der Arbeit \cite{6309496} beschrieben.

\subsubsection{Vereinfachte inverse Kinematik}
Dieser erste eigene Ansatz für eine inverse Kinematik war prototypisch für die Funktionen $f_0$ und $f_1$ gedacht. Dabei wurde das Modell stark vereinfacht. Dazu wurde die Annahme getroffen, dass die Z-Achse der Pose des Greifers $l_{5,z}$ orthogonal auf der XY-Ebene des Roboter-Koordinatensystems (im Folgenden als \textit{global} bezeichnet) steht:

\begin{equation}
l_{5,z} \parallel -Z_{Global}
\label{eq:1}
\end{equation}


Diese Annahme lässt sich auf eine konstante Ausrichtung des Greifers beim Aufheben oder Ablegen zurückführen. Dadurch ist nur noch die Rotation um die Z-Achse, sowie die Position im Kartesischen Raum, als Angaben für die Pose nötig. Dadurch wird die Pose $\xi$ aus dem $ \mathbb{R}^6 $-Raum in den $ \mathbb{R}^4 $-Raum reduziert.

Die Überlegung hinter dieser inversen Kinematik sieht vor, die Dimensionen der Pose immer weiter zu reduzieren bis das Problem mit einfachen geometrischen Sätzen zu lösen ist. Dazu wird zunächst der Mechanismus des YouBots untersucht. Dieser beinhaltet fünf Gelenke. $j_1$ und $j_5$ sind Drehgelenke um die Z-Achsen ihrer Glieder. $j_2$, $j_3$ und $j_4$ sind Drehgelenke um die X-Achsen ihrer Glieder. Das globale Koordinatensystem ist im Mittelpunkt der runden Basis des Roboters. Die Z-Achse zeigt dabei nach oben. Der Null-Punkt der Z-Achse liegt auf der Kontaktfläche zum Untergrund. Es gilt:

\begin{equation}
l_{0,z} \parallel +Z_{Global}
\label{eq:2}
\end{equation}

Aus den Gleichungen \ref{eq:1} und \ref{eq:2} folgt:

\begin{equation}
l_{0,z} \parallel l_{5,z}
\label{eq:3}
\end{equation}

Dies kann durch Betrachtung noch weiter spezifiziert werden. Da die Z-Achse vom Greifer nach $-z_{Global}$ und die Z-Achse von $l_0$ nach $+z_{Global}$  zeigt (Gleichung \ref{eq:1}) folgt:

\begin{equation}
l_{0,z} \upharpoonleft \downharpoonright l_{5,z}
\label{eq:4}
\end{equation}

Da die vierte Komponente der Pose $\alpha$ die Drehung des Greifers um die Z-Achse ist und die Drehgelenke von $j_1$ und $j_5$ in der Z-Achse parallel zu den Z-Achsen ihrer Glieder stehen ergibt sich aus den Gleichungen \ref{eq:1}, \ref{eq:2} und \ref{eq:4}:

\begin{equation}
j_1 - j_5 = \xi_{\alpha}
\end{equation}

$j_5$ geht negativ ein, weil die Z-Achse des Gelenks entgegengesetzt zur globalen Z-Achse liegt. Umgestellt nach $j_5$ ergibt sich:

\begin{equation}
j_5 = j_1 - \xi_{\alpha}
\end{equation}

Damit ist die erste Gelenkkonfiguration definiert und die Pose kann um eine Dimension, die Rotation des Greifers, reduziert werden. Da beim YouBot alle Glieder nicht in sich verdreht sind, liegen die Dreh-Achsen der Gelenke $j_2$, $j_3$ und $j_4$ parallel:

\begin{equation}
	\forall l  \in L: l_{\alpha} = 0 \implies r(j_{2}) \parallel r(j_{3}) \parallel r(j_{4}) 
\end{equation}

Die $r(j)$-Funktion extrahiert die Drehachsenkomponente des Gelenks $j$. Dadurch liegen alle Gelenke und Glieder in einer Ebene. Diese steht senkrecht auf der globalen XY-Ebene. Der Winkel der Schnittgeraden zur X-Achse ist dabei abhängig vom Drehwinkel von $j_1$. Überträgt man die X und Y Komponente der Pose aus dem kartesischen Koordinatensystem in Polarkoordinaten ergibt sich:

\begin{equation}
r = \sqrt{\xi_x^2 + \xi_y^2}
\label{eq:5}
\end{equation}

\begin{equation}
\phi = cos(\dfrac{\xi_x}{r})
\label{eq:6}
\end{equation}

Da die Kosinus-Funktion aber nicht vollständig definiert ist wird auf die $atan2$-Funktion mit zwei Parametern zurückgegriffen. Diese ist in allen gebräuchlichen Programmiersprachen vorhanden. Dadurch ergibt sich:

\begin{equation}
\phi = atan2(\xi_y,\xi_x)
\label{eq:7}
\end{equation}

$\phi$ entspricht dabei dem Drehwinkel von $j_1$. $r$ entspricht der Kombination aus $x$ und $y$. Dadurch kann man die Anzahl der Dimensionen der Pose wieder reduzieren. Diese setzt sich jetzt nur noch aus dem $z$ und dem neuen $r$ Wert zusammen. Die Abbildung \ref{fig:easyik} zeigt die nun reduzierte Umgebung. Dabei wurde der Nullpunkt des Koordinatensystems in die Position von $j_2$ verlegt. Diese Translation ist durch die Glieder $l_0$ und $l_1$ gegeben, die einen Offset entlang der globalen Z-Achse und der X-Achse von $l_1$ erzeugen. Dieser wird von dem neuen Wert $r$ abgezogen. Da $j_5$ nur um Z-Achse rotiert und es keinen Offset zwischen $l_4$ und $l_5$ gibt können diese zu dem neuen Glied $l_{45}$ zusammengefasst werden. Die Länge des Gliedes ergibt sich durch die Summe der Länge der Glieder.

\begin{equation}
a_{45} = a_{4} +a_{5}
\label{eq:8}
\end{equation}

\begin{figure}[h]
	\centering
	\subfigure[Mechanismus des Arms im globalen Koordinatensystem.]{%
		\includegraphics[scale=0.7]{fig/ikeasy1}}
	\hfill
	\subfigure[Neues Koordinatensystem mit Ursprung in $j_2$]{%
		\includegraphics[scale=0.7]{fig/ikeasy2}}
	\hfill
	\subfigure[Kennzeichnung der zu ermittelnden Winkel. Die Position $\{x_{eff}, y_{eff}\}$ von $j_4$ wird durch die Offsets von $l_1$,$l_4$ und $l_5$ berechnet.]{%
		\includegraphics[scale=0.7]{fig/ikeasy3}}
	\hfill
	\label{fig:easyik}
\end{figure}


Da $l_{45}$ senkrecht auf der X-Achse steht kann die Position $\{x_{eff}. y_{eff}\}$von $j_4$ aus der Zielpose und der Länge von $l_45$, sowie dem Offset von $l_ {1}$ berechnet werden:

\begin{equation}
x_{eff} = \xi_{x} - a_{1,x}
\label{eq:9}
\end{equation}

\begin{equation}
y_{eff} = \xi_{y} + a_{45} - a_{1,y}
\label{eq:10}
\end{equation}

Mit Hilfe des Kosinus-Satzes lassen sich nun die Winkel $\alpha_1$ und $\alpha_2$ berechnen:

\begin{equation}
l_3^2 = l_2^2 + l_{eff}^2 + 2 * l_2 * l_{eff} * cos(\alpha_1 - \alpha_{eff}) \text{ mit } l_{eff} = \sqrt{x_{eff}^2 + y_{eff}^2} \text{ und } cos(\alpha_eff) = \dfrac{x_{eff}}{l_{eff}}
\label{eq:11}
\end{equation}


\begin{equation}
l_{eff}^2 = l_2^2 + l_3^2 + 2 * l_2 * l_3 * cos(\pi - \alpha_2)
\label{eq:12}
\end{equation}

Umgestellt nach $\alpha_1$ und $\alpha_2$ ergibt sich:

\begin{equation}
\alpha_1 = \alpha_{eff} + arccos(\dfrac{l_3^2-l_2^2-l_{eff}^2}{2*l_2*l_{eff}})
\label{eq:13}
\end{equation}

\begin{equation}
\alpha_2 = \pi - arccos(\dfrac{l_{eff}^2-l_2^2-l_{3}^2}{2*l_2*l_3})
\label{eq:14}
\end{equation}

Damit sind alle Winkel bekannt, die in der Kinematik benötigt werden. Diese werden nun mit den entsprechenden Winkeloffsets addiert. Diese Offsets berechnen sich aus Grundeinstellung der Gelenke.

Diese inverse Kinematik ist, wie beschrieben sehr vereinfacht. Sie ermöglicht nur eine sehr eingeschränkte Menge an möglichen Posen. Dies ist für diese Arbeit nicht ausreichend, da für die Übergabe auch schräge Haltungen des Greifers möglich sein sollen. Auch ein Griff nach oben ist mit der bisherigen inversen Kinematik nicht möglich, da $\alpha_2 > 0$ ist. Deshalb wurde diese inverse Kinematik weiter entwickelt. Diese Weiterentwicklung beruht auf Beobachtungen der Mechanik, dabei lassen sich Überschläge bei zwei Gelenken $j_1$ und $j_3$ entdecken. Ein Überschlag bezeichnet den Wechsel der Einstellung eines Winkel $\alpha$ von $\alpha <= \pi$ zu $\alpha > \pi$. Dies kann bei $j_3$ als Ellbogen hoch oder runter beschrieben werden. Bei $j_1$ beschreibt die Vorstellung eines Griffes nach vorne oder nach hinten die Konfigurationen am besten. So zeigt der Arm nach vorne, wenn $j_1 = 0$ und $j_2 > 0$ oder $j_1 = \pi$ und $j_3 < 0$. Dies resultiert in vier möglichen Konfigurationen die der Roboterarm einnehmen kann um eine Position zu erreichen. Im besten Fall sind alle vier Konfigurationen möglich, im schlechtesten erfüllt keine Konfiguration die Pose. Die folgende inverse Kinematik versucht für die vier Konfigurationen die möglichen Gelenkkonfigurationen zu berechnen.

\subsubsection{Weiterentwicklung: Inverse Kinematik}
Die Weiterentwicklung der inversen Kinematik soll die vollständige Pose und alle möglichen Konfigurationen berechnen. Dazu reichen nicht mehr einfache Extraktionen der benötigten Komponenten, da sich diese nun gegenseitig beeinflussen. 

Zunächst wird das Koordinatensystem in den Mittelpunkt von $j_1$ gebracht. Dazu wird die Zielpose $\xi$ um den $T_0$ verschoben, diese setzt sich aus den Offsets von $l_0$ zusammen:

\begin{equation}
\xi_0 = \xi * T_0
\label{eq:15}
\end{equation}

Anschließend wird, wie in Gleichung \ref{eq:7}, $j_1$ aus der $X$ und $Y$ Komponenten Pose bestimmt.

\begin{equation}
j_1 = atan2(\xi_{0_Y},\xi_{0_X})
\label{eq:16}
\end{equation}

$j_1$ stellt eine der Möglichkeiten für das erste Gelenk dar und bildet den Griff nach \textit{vorne} ab. Der Griff nach \textit{hinten} wird durch eine Rotation um $\pi$ erreicht. Da für die Einstellung von $j_1$ gilt:

\begin{equation}
-\pi \leq j_1 \leq +\pi 
\label{eq:17}
\end{equation}

Ergibt sich für die zweite Konfiguration:
\begin{equation}
j_1 = 
\begin{cases}
j_1 + \pi & \text{f"ur } j_1  \textless 0 \\
j_1 - \pi & \text{f"ur } j_1  \ge 0 
\end{cases}
\label{eq:18}
\end{equation}

Anschließend wird die Pose wieder in ein anderes Koordinatensystem übertragen. Dabei besteht die homogene Transformation $T_1$ aus einer Translation $\upsilon_1$ und einer Rotation $\theta_1$. Zur besseren Lesbarkeit werden diese getrennt und die Rotation als Roll-Pitch-Yaw angegeben:

\begin{equation}
\pmb{\upsilon_1} = \left(\begin{array}{c} 
-a_{1_x} \\
0\\
-a_{1_z}
\end{array}\right)
,
\pmb{\theta_1} = \left(\begin{array}{c} 
0 \\
0\\
-j_1
\end{array}\right)	
,
\xi_1 = \xi_0 * T_1(\theta_1, \upsilon_1)	
\label{eq:19}
\end{equation}

Als nächstes wird die Gelenkkonfiguration für $j_5$ berechnet. Dazu wird die Pose $\xi_1$ als homogene Koordinate $A$ betrachtet. Indizes weisen auf die Stelle der Matrix:

\begin{equation}
j_5 = atan2(A_{21} * cos(j_1) - A_{11} * sin(j_1), A_{22}*cos(j_1)-A_{12} * sin(j_1))
\label{eq:27}
\end{equation}

Wie in der einfachen inversen Kinematik folgt jetzt die Transformation aus dem 3D-Raum in die 2D-Ebene des Arms. Da hier aber noch die Rotation des Greifers benötigt wird, muss zunächst diese Rotation übertragen werden. Dazu werden die Rotationskomponente der Y- und Z-Achse der Pose $\xi_1$ extrahiert:

\begin{equation}
\pmb{\hat{\theta}_y} = \xi_{1_{RotY}}, \pmb{\hat{\theta}_z} = \xi_{1_{RotZ}}  
\label{eq:20}
\end{equation}

Für die weiteren Schritte wird die Normale der Armebene $\pmb{\hat{m}}$ benötigt:

\begin{equation}
\pmb{\hat{m}} = 
\left(\begin{array}{c} 
0 \\
-1\\
0
\end{array}\right)
\label{eq:21}
\end{equation}

Aus dieser und der Z-Rotation lässt sich nun der Rotationsvektor $\pmb{\hat{k}}$ bestimmen um den die Endpose für die Rotation gedreht werden muss:

\begin{equation}
\pmb{\hat{k}} = \pmb{\hat{m}} \times \pmb{\hat{\theta}_z} 
\label{eq:22}
\end{equation}

Dadurch ergibt sich für die Z-Achsen Rotation der neuen Pose $\xi_1'$:

\begin{equation}
\pmb{\hat{\theta}_z'} = \pmb{\hat{k}} \times \pmb{\hat{m}}
\label{eq:23}
\end{equation}

Als nächstes wird die Rotation um die Y-Achse der Pose $\pmb{\hat{\theta}_y'}$ berechnet. Dazu wird die Rodrigues-Formel angewandt. Dafür müssen zunächst die $cos_\theta$ und $sin_\theta$ Werte berechnet werden. Diese lassen sich mit Hilfe des Kreuz- und Skalarproduktes bestimmen:

\begin{equation}
cos_\theta = \pmb{\hat{\theta}_z} \bullet \pmb{\hat{\theta}_z'}, sin_\theta = (\pmb{\hat{\theta}_z} \times \bullet \pmb{\hat{\theta}_z'}) \bullet \pmb{\hat{k}}
\label{eq:24}
\end{equation}

Rodrigues angewandt:

\begin{equation}
\pmb{\hat{\theta}_y'} = cos_\theta * \pmb{\hat{\theta}_y} + sin_\theta * ( \pmb{\hat{k}} \times \pmb{\hat{\theta}_y}) + (1 - cos_\theta) * (\pmb{\hat{k}} \bullet \pmb{\hat{\theta}_y}) * \pmb{\hat{k}} 
\label{eq:25}
\end{equation}

Da $\pmb{\hat{\theta}_y'}$ und $\pmb{\hat{\theta}_z'} $ nun bekannt sind lässt sich aus dem Kreuzprodukt die X-Rotation $\pmb{\hat{\theta}_x'} $ bestimmen:

\begin{equation}
\pmb{\hat{\theta}_x'} = \pmb{\hat{\theta}_y'} \times \pmb{\hat{\theta}_z'}
\label{eq:26}
\end{equation}

Diese drei Komponenten beinhalten die projizierte Rotation der Pose $\xi_1'$. Die Position der Pose bleibt erhalten. Als nächstes wird das Nicken des Greifers berechnet. Dieser ergibt sich aus der Summer der Winkel von $j_2$, $j_3$ und $j_4$. Dazu wird, wie bei der Berechnung zu $j_5$, der Pose als homogene Koordinate $A'$ betrachtet und zunächst $x_{eff}$ und $y_{eff}$ aus der projizierten Pose extrahiert und der Gesamtwinkel $\alpha_{eff}$ berechnet:

\begin{equation}
x_{eff} = A'_{13}, y_{eff} = A'_{13}, \alpha_{eff} = atan2(y_{eff}, x_{eff})
\label{eq:28}
\end{equation}

Daraus lässt sich nun die Position von $j_4$ berechnen. Dazu wird die Summe der Länge von $l_4$ und $l_5$ zusammengefasst $d_{45} = l_4 + l_5$. Die neue Position berechnet sich nun aus:

\begin{equation}
j_{4_{pos}} = 
\left(\begin{array}{c} 
\xi_{1_x}' - d_{45} * sin(\alpha_{eff})\\
\xi_{1_y}' - d_{45} * cos(\alpha_{eff})
\end{array}\right)
\label{eq:29}
\end{equation}

\begin{equation}
|j_{4_{pos}}| = \sqrt{j_{4_{pos_x}}^2 + j_{4_{pos_y}}^2}
\label{eq:31}
\end{equation}

Nun lässt sich schon bestimmen, ob die Zielpose nicht im Arbeitsraum des Roboters liegt. Die Zielpose liegt nicht im Arbeitsraum, wenn:

\begin{equation}
a_2 + a_3 < |j_{4_{pos}}|
\label{eq:30}
\end{equation}

Nachdem nun die beiden Endgelenke $j_1$ und $j_5$ berechnet wurden, wird mit dem mittleren Gelenk $j_3$ fortgefahren. Dies begründet sich durch den zweiten Konfigurationsschritt Ellbogen \textit{hoch} oder \textit{runter}. Diese Konfiguration ist Richtungsvorgabe, in welche sich das Gelenk rotieren soll (positiv oder negativ). Dazu wird zunächst mit dem Kosinussatz der Kosinus von $j_3$ ermittelt:

\begin{equation}
cos(j_3) = \dfrac{s_E - a_2^2 - a_3^2}{2 * a_2 * a_3} \text{ mit } s_E = j_{4_{pos_x}}^2 + j_{4_{pos_y}}^2 
\label{eq:32}
\end{equation}

Da die $acos$-Funktion nur innerhalb vom Wertebereich $[-1;1]$ definiert ist und generell die $atan2$-Funktion genutzt werden soll ergibt sich folgende Fallunterscheidung für $j_3$:

\begin{equation}
j_3 = 
\begin{cases}
0  & \text{f"ur } cos(j_3) \textgreater 0.9999 \\
\pi & \text{f"ur } cos(j_3) \textless -0.9999 \\
atan2(\sqrt{1 - cos(j_3)^2}, cos(j_3))  & \text{f"ur } Rest
\end{cases}
\label{eq:33}
\end{equation}

Nun kann je nach Konfiguration der Winkel positiv $j_3 = j_3$ oder negativ $j_3 = -j_3$ betrachtet werden. Er wird für die Berechnung von $j_2$ und $j_4$ weitergenutzt. Dazu wird vom Vollkreis ($2\pi$) der der Winkel zwischen den Gelenken $l_2$ und $l_3$ abgezogen:

\begin{equation}
j_2 = 2\pi - atan2(j_{4_{pos_y}}, j_{4_{pos_x}}) - atan2(a_3 * sin(j_3), a_2 + a_3 * cos(j_3))
\label{eq:34}
\end{equation}

Das letzte fehlende Gelenk $j_2$ ergibt sich aus der dem Nicken des Greifers $\alpha_{eff}$, abzüglich der Gelenke $j_3$ und $j_2$:

\begin{equation}
j_4 =  \alpha_{eff} - j_2 - j_3
\label{eq:345}
\end{equation}

Auf alle Winkel muss noch ein Offset addiert werden, da die Grundeinstellung der inversen Kinematik anders ist als bei der Grundeinstellung der Ansteuerung. Bei der Ansteuerung befinden sich alle Gelenke bei null Grad, wenn der Arm in der \textit{Fold}-Pose ist, die inverse Kinematik geht von der \textit{Candle}-Pose als Grundstellung aus. Die vollständige inverse Kinematik wird im folgenden als $Ik(\xi, c)$-Funktion genutzt. Dabei ist $\xi$ die Pose und $c$ die Konfigurationsvariante aus der Menge der Konfigurationen $C$. Für diese inverse Kinematik besteht die Menge aus vier Elementen die abstrakt beschrieben werden können mit: \textit{Vorne-Ellbogen-hoch}, \textit{Vorne-Ellbogen-runter}, \textit{Hinten-Ellbogen-hoch} und \textit{Hinten-Ellbogen-runter}.

\subsubsection{Optimierte Pfadplanung}
Neben der inversen Kinematik ist auch die Pfadplanung wichtig. Da die inverse Kinematik maximal vier mögliche Konfigurationen zurückgibt, muss die Verkettung der Posenübergänge optimiert werden, um möglichst geringe Kosten (Bewegungsenergie und Zeit) zu erreichen. Durch Betrachtung ergibt sich, dass Posenübergänge besonders teuer sind, wenn die Konfiguration gewechselt muss. Dabei ist die Basisrotation, $j_1$ betreffend, teurer, als die Ellbogen-Konfiguration, $j_3$. Dies liegt an dem Einfluss den eine Konfiguration hat. So beeinflusst eine Konfigurationsänderung von $j_1$ alle anderen Winkel. Eine Änderung an der Konfiguration von $j_3$ beeinflusst dagegen nur die Gelenk $j_2$ und $j_4$. Die Kosten $K$ für eine Bewegung ermittelt sich aus den Einstellungen der Winkel bei der Start- und der Zielpose:

\begin{equation}
K(J_{start}, J_{ziel}) = \sum_{i=1}^{n} (J_{start_i} - J_{ziel_i})^2
\label{eq:36}
\end{equation}

Für eine Bewegung $\Gamma$, die aus $n$ Posen besteht $\Gamma: \xi_0 \oplus \xi_1 \oplus ... \oplus \xi_n$, gilt auf Grund der Beobachtung folgende Lösung als optimal ($\xi_0$ entspricht der aktuellen Pose):

\begin{equation}
K_{min} = \min_{c \in C}(\sum_{i=0}^{n-1} K(Ik(\xi_i, c), Ik(\xi_{i+1}, c))
\label{eq:37}
\end{equation}

Dies funktioniert nur solange alle Posen innerhalb einer Konfiguration erreicht werden können. Ist dies nicht möglich muss die günstigste Alternative gefunden werden. Für dieses Problem eignet sich die Graphentheorie. Die Knoten entsprechen dabei allen Gelenkeinstellungen aus der Kombination der Posen und der Konfigurationen der inversen Kinematik. Die Kanten repräsentieren die Kosten für den Übergang zwischen zwei Gelenkeinstellungen. Jeder Knoten besitzt gerichtete Kanten zu den Knoten die auf der folgenden Pose basieren. Am Übersichtlichsten ist der Graph, wenn Knoten basierend auf der gleichen Pose nebeneinander und Knoten mit der gleichen Konfiguration untereinander angeordnet werden. Exemplarisch ist ein solcher Graph in Abbildung \ref{fig:plangraph} dargestellt. Dabei sind die Kantengewichtungen aus Gründen der Lesbarkeit ausgelassen worden. Nur zwei Kanten in blau stellen die Beziehungen exemplarisch dar. $J_0$ beschreibt die Gelenkkonfiguration vor Bewegungsbeginn. Die Pose und die Konfiguration sind dabei unbekannt. Existiert für eine Kombination aus Pose und Konfiguration keine Gelenklösung entfällt der dazugehörige Knoten und Kanten. Gesucht wird nun der günstigste Weg von $J_0$ zur untersten Ebene.

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.8]{fig/graphplan}
	\caption[Graph zur Pfadplanung]{Der dargestellte Graph bildet die Planung des Bewegungspfad ab. Knoten stellen eine Gelenkeinstellung zu einer Pose und einer Konfiguration. Dabei werden Knoten mit der gleichen Konfiguration senkrecht und mit der gleichen Pose waagerecht angeordnet. Aufeinander folgende Posen sind untereinander sortiert. Die Kanten werden mit den Übergangskosten gewichtet. Zur Übersichtlichkeit wurden nur zwei Kanten exemplarisch in blau gekennzeichnet.}
	\label{fig:plangraph}
\end{figure}

\subsubsection{Nachtrag Literatur}
Nach der Entwicklung und der Niederschrift über die inverse Kinematik wurde im Rahmen einer erweiterten Literaturrecherche das Paper \cite{6309496} gesichtet, das eine ähnliche inverse Kinematik beschreibt. Diese geht auch von einer geometrischen Lösung aus. Der Unterschied bei der Bestimmung ist vor allem die Einbindung der mobilen Plattform in die inverse Kinematik. Dies ist für den stationären Arm unnötig und kann zwar auch aus der Gleichung entfernt werden. Dennoch wurde entschieden, dass die eigene inverse Kinematik genutzt werden soll. Dies ist vor allem durch die Wartbarkeit und Änderbarkeit, aber auch durch Interesse an der eigenen Arbeit begründet.
